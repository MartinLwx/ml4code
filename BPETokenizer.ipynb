{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d91bf5-03b1-48b0-9e61-a578e7a4b9b8",
   "metadata": {},
   "source": [
    "## What is the BPE tokenization?\n",
    "\n",
    "### BPE training phase\n",
    "\n",
    "Now let's figure out how the BPE tokenizer got trained. Let's assume that we have some documents $D$\n",
    "1. For each $d$, we will transform the documents into word list *in some way*. *For instance, you may choose to split the document by whitespace to get words*.\n",
    "2. Count the word freq for each word $w$ in $D$, and we can also get the alphabet of $D$ as the initial vocab(plus the `</w>`)\n",
    "3. For each word, transform the word into a utf-8 char list. We call it a split. *For example, `highest -> h, i, g, h, e, s, t`*\n",
    "4. Append `</w>` to each utf-8 list. *e.g. `highest -> h, i, g, h, e, s, t, </w>`*\n",
    "5. Repeat the following steps until any one of the two conditions is met: 1) Vocab reaches the upper limit. 2) Reach the maximum number of iterations\n",
    "    1. Find **the most frequent pair**, add it to a merge table, and add the merged result to the vocab\n",
    "    2. Update all splits of all words. *For example, the most frequent pair may be `(h, i)` in our previous example, then we will do `highest -> hi, g, h, e, s, t, </w>`*\n",
    "\n",
    "You may have 3 puzzles:\n",
    "1. Why word frequency? Because we want to find the most frequent pair easily\n",
    "2. Why append `</w>`? Because we want to reconstruct the input later, we use `</w>` to mark that it is the end of a word\n",
    "3. What if we have multiple pairs with the same frequency? How to handle this may vary in different implementations, but *shouldn't* have much impact in my opinion.\n",
    "\n",
    "> ðŸ’¡ You can observe that when the BPE algorithm merges the most frequently occurring pair, it doesn't cross over words.\n",
    "\n",
    "### How to use a trained BPE?\n",
    "\n",
    "After we trained a BPE tokenizer, we will obtain a merge table and a vocab. Assuming that we now need to tokenize the text `s`\n",
    "\n",
    "1. Use the same method as during training, start by splitting `s` into individual words, with each word further divided into utf-8 char.\n",
    "2. Iterate through the merge table and check if each merge rule can be applied to update the split of each word.\n",
    "\n",
    "> ðŸ’¡ An important detail here is that the merge rules we extracted are sorted in descending order of frequency. Thus, by sequentially traversing the merge table, we are *implicitly* incorporating the notion of prioritizing the merging of the most frequently occurring pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c72a5",
   "metadata": {},
   "source": [
    "## BPE in practice\n",
    "\n",
    "The API provided by the Huggingface is quite simple. *You may notice that it uses `Char` in the class name, which confirms what I mentioned earlier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2142e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_code\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484fe42a-70dd-448d-ab2c-a3dc400df721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the \"test\" dataset to speed up \"training\"\n",
    "corpus = get_code(\"test\", language=\"go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba8dc1-d974-4769-8875-ddb356eca7ae",
   "metadata": {},
   "source": [
    "To create t BPE Tokenizer, we leverage the `CharBPETokenizer` provided by the Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1191de-b7b7-4e41-baef-c34da4842064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = CharBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795a916b-2581-466d-ade7-b66f23e69429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=256):\n",
    "    for i in range(0, len(corpus), batch_size):\n",
    "        yield corpus[i: i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708175d2-c7bd-49ac-9c8a-6062303b719a",
   "metadata": {},
   "source": [
    "We just need to call `tokenizer.train_from_iterator` method here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd4f447-a59b-4cab-9374-8a54d7cc4994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    batch_iterator(),\n",
    "    vocab_size=50265,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec53d4-43cd-4218-8621-2ee223e763b9",
   "metadata": {},
   "source": [
    "Now let's grab one random sample code from the corpus and see the tokenization result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a649596d-a228-436a-a5da-eacc6fc517a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func mustWaitPinReady(t *testing.T, cli *clientv3.Client) {\n",
      "\t// TODO: decrease timeout after balancer rewrite!!!\n",
      "\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n",
      "\t_, err := cli.Get(ctx, \"foo\")\n",
      "\tcancel()\n",
      "\tif err != nil {\n",
      "\t\tt.Fatal(err)\n",
      "\t}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sample = corpus[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f3966d-da56-49e2-b885-ff6d51157ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['func</w>', 'must', 'Wait', 'Pin', 'Ready</w>', '(</w>', 't</w>', '*</w>', 'testing</w>', '.</w>', 'T</w>', ',</w>', 'cli</w>', '*</w>', 'clientv3</w>', '.</w>', 'Client</w>', ')</w>', '{</w>', '/</w>', '/</w>', 'TODO</w>', ':</w>', 'decrease</w>', 'timeout</w>', 'after</w>', 'balancer</w>', 'rewrite</w>', '!</w>', '!</w>', '!</w>', 'ctx</w>', ',</w>', 'cancel</w>', ':</w>', '=</w>', 'context</w>', '.</w>', 'WithTimeout</w>', '(</w>', 'context</w>', '.</w>', 'Background</w>', '(</w>', ')</w>', ',</w>', '10</w>', '*</w>', 'time</w>', '.</w>', 'Second</w>', ')</w>', '_</w>', ',</w>', 'err</w>', ':</w>', '=</w>', 'cli</w>', '.</w>', 'Get</w>', '(</w>', 'ctx</w>', ',</w>', '\"</w>', 'foo</w>', '\"</w>', ')</w>', 'cancel</w>', '(</w>', ')</w>', 'if</w>', 'err</w>', '!</w>', '=</w>', 'nil</w>', '{</w>', 't</w>', '.</w>', 'Fatal</w>', '(</w>', 'err</w>', ')</w>', '}</w>', '}</w>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(sample).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0516e0-a5c8-48fa-b780-650cdc08a064",
   "metadata": {},
   "source": [
    "According to the tokenization result, we find some interesting things about the BPE Tokenizer\n",
    "- It learns how to **split function/variable name in camelCase automatically**. *`mustWaitPinReady` -> `['must', 'Wait', 'Pin', 'Ready</w>']`*\n",
    "- It also **keeps the meaningful keyword of go language**. *`func` means a function declaration in go*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7594d-9a9a-444f-8c5f-d2a247f5f5b0",
   "metadata": {},
   "source": [
    "## Implement a BPE Tokenizer\n",
    "\n",
    "To get a better understanding of the BPE tokenizer, we can try to implement one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1f91c7-4e84-4d93-a66a-7c6a224f9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: list[str],\n",
    "        vocab_size: int,\n",
    "        max_iter: int | None = None,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = []\n",
    "        self.word_freq = Counter()\n",
    "        self.splits = {}  # e.g. highest: [high, est</w>]\n",
    "        self.merges = {}  # e.g. [high, est</w>]: highest\n",
    "        self.max_iter = max_iter\n",
    "        self.debug = debug\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train a BPE Tokenizer\"\"\"\n",
    "        # count the word frequency\n",
    "        for document in self.corpus:\n",
    "            # split each document in corpus by whitespace\n",
    "            words = document.split()\n",
    "            self.word_freq += Counter(words)\n",
    "\n",
    "        # intialize the self.splits\n",
    "        for word in self.word_freq:\n",
    "            self.splits[word] = list(word) + [\"</w>\"]\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Init splits: {self.splits}\")\n",
    "\n",
    "        alphabet = set()\n",
    "        for word in self.word_freq:\n",
    "            alphabet |= set(list(word))\n",
    "        alphabet.add(\"</w>\")\n",
    "\n",
    "        self.vocab = list(alphabet)\n",
    "        self.vocab.sort()\n",
    "\n",
    "        cnt = 0\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            if self.max_iter and cnt >= self.max_iter:\n",
    "                break\n",
    "\n",
    "            # find the most frequent pair\n",
    "            pair_freq = self.get_pairs_freq()\n",
    "\n",
    "            if len(pair_freq) == 0:\n",
    "                print(\"No pair available\")\n",
    "                break\n",
    "\n",
    "            pair = max(pair_freq, key=pair_freq.get)\n",
    "\n",
    "            self.update_splits(pair[0], pair[1])\n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"Updated splits: {self.splits}\")\n",
    "\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "\n",
    "            self.vocab.append(pair[0] + pair[1])\n",
    "\n",
    "            if self.debug:\n",
    "                print(\n",
    "                    f\"Most frequent pair({max(pair_freq.values())} times) \"\n",
    "                    f\"is : {pair[0]}, {pair[1]}. Vocab size: {len(self.vocab)}\"\n",
    "                )\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "    def update_splits(self, lhs: str, rhs: str):\n",
    "        \"\"\"If we see lhs and rhs appear consecutively, we merge them\"\"\"\n",
    "        for word, word_split in self.splits.items():\n",
    "            new_split = []\n",
    "            cursor = 0\n",
    "            while cursor < len(word_split):\n",
    "                if (\n",
    "                    word_split[cursor] == lhs\n",
    "                    and cursor + 1 < len(word_split)\n",
    "                    and word_split[cursor + 1] == rhs\n",
    "                ):\n",
    "                    new_split.append(lhs + rhs)\n",
    "                    cursor += 2\n",
    "                else:\n",
    "                    new_split.append(word_split[cursor])\n",
    "                    cursor += 1\n",
    "            self.splits[word] = new_split\n",
    "\n",
    "            # if word_split != new_split:\n",
    "            #     print(f\"old: {word_split}\")\n",
    "            #     print(f\"new: {new_split}\")\n",
    "\n",
    "    def get_pairs_freq(self) -> dict:\n",
    "        \"\"\"Compute the pair frequency\"\"\"\n",
    "        pairs_freq = defaultdict(int)\n",
    "        for word, freq in self.word_freq.items():\n",
    "            split = self.splits[word]\n",
    "            for i in range(len(split)):\n",
    "                if i + 1 < len(split):\n",
    "                    pairs_freq[(split[i], split[i + 1])] += freq\n",
    "\n",
    "        return pairs_freq\n",
    "\n",
    "    def tokenize(self, s: str) -> list[str]:\n",
    "        splits = [list(t) + [\"</w>\"] for t in s.split()]\n",
    "\n",
    "        for lhs, rhs in self.merges:\n",
    "            for idx, split in enumerate(splits):\n",
    "                new_split = []\n",
    "                cursor = 0\n",
    "                while cursor < len(split):\n",
    "                    if (\n",
    "                        cursor + 1 < len(split)\n",
    "                        and split[cursor] == lhs\n",
    "                        and split[cursor + 1] == rhs\n",
    "                    ):\n",
    "                        new_split.append(lhs + rhs)\n",
    "                        cursor += 2\n",
    "                    else:\n",
    "                        new_split.append(split[cursor])\n",
    "                        cursor += 1\n",
    "                assert \"\".join(new_split) == \"\".join(split)\n",
    "                splits[idx] = new_split\n",
    "\n",
    "        return sum(splits, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636d0ff-efde-44bf-91ca-1fdde73e5d10",
   "metadata": {},
   "source": [
    "Let's use the subset of `corpus` for training because the Python implementation is inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ca6943-70b7-4225-bc05-6126ea0044ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE(corpus[:200], vocab_size=2000, debug=False)\n",
    "bpe.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e852cc-2bf2-40db-b806-4913965a240a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['func</w>', 'must', 'W', 'a', 'it', 'P', 'in', 'Read', 'y', '(t</w>', '*', 'testing.T', ',</w>', 'cli', '</w>', '*', 'client', 'v3.Client', ')</w>', '{</w>', '//</w>', 'TODO:</w>', 'de', 'c', 'rea', 'se</w>', 'time', 'out</w>', 'after</w>', 'bal', 'an', 'c', 'er</w>', 're', 'write', '!', '!', '!', '</w>', 'ctx,</w>', 'cancel</w>', ':=</w>', 'context.With', 'Timeout', '(context.Background', '(),</w>', '1', '0', '*', 'time.Second)</w>', '_,</w>', 'err</w>', ':=</w>', 'cli', '.Get', '(ctx,</w>', '\"', 'foo', '\")</w>', 'cancel()</w>', 'if</w>', 'err</w>', '!=</w>', 'nil</w>', '{</w>', 't.Fatal(err)</w>', '}</w>', '}</w>']\n"
     ]
    }
   ],
   "source": [
    "print(bpe.tokenize(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd586591-a422-49db-bcd6-5d5786bb4329",
   "metadata": {},
   "source": [
    "Let's do a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d67bb7ce-52cc-4ce7-9149-fbc5cbc378cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['func</w>', 'must', 'Wait', 'Pin', 'Ready</w>', '(</w>', 't</w>', '*</w>', 'testing</w>', '.</w>', 'T</w>', ',</w>', 'cli</w>', '*</w>', 'clientv3</w>', '.</w>', 'Client</w>', ')</w>', '{</w>', '/</w>', '/</w>', 'TODO</w>', ':</w>', 'decrease</w>', 'timeout</w>', 'after</w>', 'balancer</w>', 'rewrite</w>', '!</w>', '!</w>', '!</w>', 'ctx</w>', ',</w>', 'cancel</w>', ':</w>', '=</w>', 'context</w>', '.</w>', 'WithTimeout</w>', '(</w>', 'context</w>', '.</w>', 'Background</w>', '(</w>', ')</w>', ',</w>', '10</w>', '*</w>', 'time</w>', '.</w>', 'Second</w>', ')</w>', '_</w>', ',</w>', 'err</w>', ':</w>', '=</w>', 'cli</w>', '.</w>', 'Get</w>', '(</w>', 'ctx</w>', ',</w>', '\"</w>', 'foo</w>', '\"</w>', ')</w>', 'cancel</w>', '(</w>', ')</w>', 'if</w>', 'err</w>', '!</w>', '=</w>', 'nil</w>', '{</w>', 't</w>', '.</w>', 'Fatal</w>', '(</w>', 'err</w>', ')</w>', '}</w>', '}</w>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(sample).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b4393-7fab-44c1-a554-612cb5c69bbd",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "The BPE tokenize is simple and practical, but **when you delve into its implementation, you will encounter several details. However, it's precisely by engaging with these intricacies that your understanding of BPE becomes more profound**.\n",
    "\n",
    "Let's also talk about some limitations of BPE. For instance, you will notice that we are using whitespace to split text, which **works for whitespaced language**. However, for languages like Chinese, spaces don't define word boundaries, which makes things more complex and calls for a better tokenizing method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
