{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af8a87d-de4d-4e3c-9e1e-55e4aae89618",
   "metadata": {},
   "source": [
    "## What is the TF-IDF model?\n",
    "\n",
    "Previously, we talked about the bag-of-word model, which has many limitations. Today we take a step further to see if we can try to fix **one of the limitations - Each word has the same importance**.\n",
    "\n",
    "> ðŸ’¡ The crux of the problem - **How to define the word importance**?\n",
    "\n",
    "One idea is: The more frequently a word appears **within a single document**, the more important it is **for that document**. *For instance, in an article discussing dogs, the word \"dog\" is likely to appear frequently, reflecting the document's main topic*.\n",
    "\n",
    "But what if a word appears very frequently **in all documents**? *For example, the word \"of\" may appear quite often in every document, can we say \"of\" is important?* Clearly, that's not the case. So we have a clue here: If a word has a high frequency in **every document**, probably it's not significant and does not convey too much information.\n",
    "\n",
    "Therefore, a reasonable solution should consider a word's frequency within **a single document** but also take into account its frequency crossing **multiple documents**. TF-IDF balances these two aspects.\n",
    "\n",
    "In summary, the **intuition** behind TF-IDF is - **Similar documents *may* use similar words, while the importance of different words should vary**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53751ac0-13fa-410d-8776-0596630163e7",
   "metadata": {},
   "source": [
    "## TF-IDF model in detail\n",
    "\n",
    "TF-IDF = Term frequency(TF) + Inverse document frequency(IDF). Let's break it into two parts\n",
    "### TF\n",
    "\n",
    "The TF can be seen as a function of document $d$ and word $w$, and the equation is:\n",
    "\n",
    "$$\\text{TF}(w, d)=\\frac{\\text{frequency of}\\ w\\ \\text{in}\\ d}{\\text{word counts of } d}$$\n",
    "\n",
    "That is, we just need to calculate the frequency of word $w$ in the document $d$, and then divide it by the total number of words in $d$.\n",
    "\n",
    "> ðŸ› In Scikit-Learn, the computation of TF is a bit different. It doesn't involve dividing by the total number of words in the document. The purpose of dividing is to normalize the $\\text{TF}(w, d)$ values within the document $d$, making them add up to one. In Scikit-Learn, this normalization process is performed after the TF-IDF calculation. We will demonstrate this with an example later.\n",
    "\n",
    "### IDF\n",
    "\n",
    "The goal of IDF is to reduce the importance of some common words that appear in each document. Therefore, the IDF is a function involving the word $w$ and the $corpus$.\n",
    "\n",
    "$$\n",
    "\\text{IDF}(w, corpus)=log\\ \\frac{\\text{document count of }corpus}{1+\\text{count of document which contains }w}\n",
    "$$\n",
    "\n",
    "We add one in the denominator to avoid division by 0.\n",
    "\n",
    "> ðŸ¤”ï¸ The $corpus$ is gennerally fixed. So it can be treated as a constant. In that case, IDF can be considered as something that's only related to the word $w$\n",
    "\n",
    "> ðŸ’¡ Note the $log$ here. Are we using $log_2$, $log_{10}$ or $ln$? Different frameworks might have variations. *Scikit-Learn use $ln$*\n",
    "\n",
    "> ðŸ› In Scikit-Learn, the calculation of IDF differs from the equations mentioned above. By default, Scikit-Learn uses the following formula[^1]:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(w, corpus)=log\\ \\frac{1 + \\text{document count of }corpus}{1+\\text{count of document which contains }w} + 1\n",
    "$$\n",
    "\n",
    "ðŸ¤”ï¸ In my opinion, **the modification made by Scikit-Learn ensures that $\\text{IDF}(w)$ cannot be less than 1**. In the origin equation, if a word $w$ appears in each document within the corpus, $\\text{IDF}(w)$ could be a negative value. Therefore, Scikit-Learn's modification seems more practical. It provides a more intuitive comparison of the IDF values for different words.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(w, d, corpus)=\\text{TF}(w, d) * \\text{IDF}(d, corpus)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f80dcc-4337-406b-9c90-86e0ab11a05b",
   "metadata": {},
   "source": [
    "## The TF-IDF in Scikit-Learn\n",
    "\n",
    "It's trivial to implement the TF-IDF algorithm. However, probably you will just use the well-established APIs provided by Scikit-Learn. Here, we will delve into how to calculate TF-IDF in Scikit-Learn. Let's proceed by continuing to use the official example from Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7deb7703-f896-400c-9d2b-17e8ab6ccfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "tokenized_toy_corpus = [\n",
    "    ['this', 'is', 'the', 'first', 'document'],\n",
    "    ['this', 'is', 'the', 'second', 'second', 'document'],\n",
    "    ['and', 'the', 'third', 'one'],\n",
    "    ['is', 'this', 'the', 'first', 'document']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014e9bd-20de-4c82-ae2d-2799868ed168",
   "metadata": {},
   "source": [
    "Let's retrieve the TF-IDF matrix using the APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8e9cc3-a594-419b-bcd3-682149728322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# set norm=None for comparison\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "X = vectorizer.fit_transform(toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a2d884-7b86-4575-94cc-422e77caf990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66aebfc-4ff7-4884-944a-0325a73ea040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.22314355 1.51082562 1.22314355 0.         0.\n",
      "  1.         0.         1.22314355]\n",
      " [0.         1.22314355 0.         1.22314355 0.         3.83258146\n",
      "  1.         0.         1.22314355]\n",
      " [1.91629073 0.         0.         0.         1.91629073 0.\n",
      "  1.         1.91629073 0.        ]\n",
      " [0.         1.22314355 1.51082562 1.22314355 0.         0.\n",
      "  1.         0.         1.22314355]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5b713-cbc7-4702-b30b-69a48e4f7c28",
   "metadata": {},
   "source": [
    "We can access the TF-IDF matrix using `X.toarray()` (Note that I set `norm=None` in the code snippet)\n",
    "\n",
    "|           | and     | document | first   | is      | one     | second  | the | third   | this    |\n",
    "| --------- | ------- | -------- | ------- | ------- | ------- | ------- | --- | ------- | ------- |\n",
    "| document1 | 0.0     | 1.22314  | 1.51082 | 1.22314 | 0.0     | 0.0     | 1.0 | 0.0     | 1.22314 |\n",
    "| document2 | 0.0     | 1.22314  | 0.0     | 1.22314 | 0.0     | 3.83258 | 1.0 | 0.0     | 1.22314 |\n",
    "| document3 | 1.91629 | 0.0      | 0.0     | 0.0     | 1.91629 | 0.0     | 1.0 | 1.91629 | 0.0     |\n",
    "| document4 | 0.0     | 1.22314  | 1.51082 | 1.22314 | 0.0     | 0.0     | 1.0 | 0.0     | 1.22314 |\n",
    "\n",
    "Let me also put the bag-of-word matrix here:\n",
    "\n",
    "|           | and | document | first | is  | one | second | the | third | this |\n",
    "| --------- | --- | -------- | ----- | --- | --- | ------ | --- | ----- | ---- |\n",
    "| document1 | 0   | 1        | 1     | 1   | 0   | 0      | 1   | 0     | 1    |\n",
    "| document2 | 0   | 1        | 0     | 1   | 0   | 2      | 1   | 0     | 1    |\n",
    "| document3 | 1   | 0        | 0     | 0   | 1   | 0      | 1   | 1     | 0    |\n",
    "| document4 | 0   | 1        | 1     | 1   | 0   | 0      | 1   | 0     | 1    |\n",
    "\n",
    "ðŸ¤”ï¸ *Comparing these two matrices, we can find that the word importance of `document` and `first` inside the document1 has changed. The `TF-IDF` value for `document` is `1.22314`, while the TF-IDF value for `first` is `1.51082`, due to the unequal presence of these words in the corpus. However, the bag-of-word model fails to recognize this and considers both of them as having an importance of `1`*\n",
    "\n",
    "We can retrieve the IDF value of each word by accessing the `idf_` attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83fe64a4-5c36-4562-97c6-d0856cf234bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.22314355 1.91629073 1.91629073\n",
      " 1.         1.91629073 1.22314355]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf8b30-564f-4d55-b799-64a812255ade",
   "metadata": {},
   "source": [
    "ðŸ¤”ï¸ If we multiply this IDF vector by the matrix output of the bag-of-word model(note that the IDF vector will be broadcasted), you would obtain the TF-IDF matrix calculated by Scikit-Learn. This confirms what we mentioned earlier:\n",
    "- Scikit-Learn directly uses the output of the bag-of-word model as TF.\n",
    "- Scikit-Learn's IDF calculation differs from the standard approach.\n",
    "\n",
    "\n",
    "## Implement TF-IDF manually\n",
    "\n",
    "We assume that each document within the corpus is tokenized, and we use the TF-IDF definition of Scikit-Learn\n",
    "\n",
    "> ðŸ› The code below is not optimized, just for demonstration :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb3e581-a3a9-412f-bf46-a8db353a07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def TF(word: str, tokenized_document: list[str]) -> float:\n",
    "    return tokenized_document.count(word)\n",
    "\n",
    "\n",
    "def IDF(word: str, tokenized_corpus: list[list[str]]) -> float:\n",
    "    doc_count_contains_word = 0\n",
    "    for doc in tokenized_corpus:\n",
    "        if word in doc:\n",
    "            doc_count_contains_word += 1\n",
    "\n",
    "    return math.log((1 + len(tokenized_corpus)) / (1 + doc_count_contains_word)) + 1\n",
    "\n",
    "\n",
    "def TF_IDF(\n",
    "    word: str, tokenized_document: list[str], tokenized_corpus: list[list[str]]\n",
    ") -> float:\n",
    "    return TF(word, tokenized_document) * IDF(word, tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4a6c8-259d-4478-a105-f037647ecdac",
   "metadata": {},
   "source": [
    "## TF-IDF for CodeSearchNet\n",
    "\n",
    "Let's use the TF-IDF model to generate the feature vector for each code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858f705a-bbff-48e0-975a-721947ad3717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import get_code, get_token_stream\n",
    "from tqdm.auto import tqdm\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "defd2813-1cb8-427f-ba78-8974b3aefb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_code(\"test\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0467130d-7804-45fe-bbb3-384fffe16db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22176/22176 [00:13<00:00, 1626.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python2: 228, Python3: 21948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "py2_cnt, py3_cnt = 0, 0\n",
    "new_corpus = []\n",
    "codes = []\n",
    "for code in tqdm(corpus):\n",
    "    try:\n",
    "        codes.append(get_token_stream(code))\n",
    "        new_corpus.append(code)\n",
    "        py3_cnt += 1\n",
    "    except SyntaxError:\n",
    "        py2_cnt += 1\n",
    "print(f\"Python2: {py2_cnt}, Python3: {py3_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9feb0004-ee47-409c-9991-8f6c26d50d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "834b61a5-c3b4-48b6-8dc3-111d49adb34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<31933 unique tokens: ['', '(', ')', ',', ':']...>\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(codes)\n",
    "\n",
    "once_ids = [\n",
    "    token_id\n",
    "    for token_id, doc_freq in dictionary.dfs.items()\n",
    "    if doc_freq == 1\n",
    "]\n",
    "\n",
    "dictionary.filter_tokens(once_ids)\n",
    "dictionary.compactify()\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe207e92-12aa-493d-a154-78661b523cbc",
   "metadata": {},
   "source": [
    "First, let's get the bag-of-word matrix first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c212390-dc7b-40d9-936d-ecb55953d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW_matrix_for_code = [dictionary.doc2bow(d) for d in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0583eeae-c13b-4d4d-9750-9af6a806d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4db6a70-b741-4861-a056-3f0ed6ee77cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = TfidfModel(BoW_matrix_for_code, dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b1ccd-8970-4315-9643-305d9866cbf7",
   "metadata": {},
   "source": [
    "Now we have built the tf-idf model, we can use it to get tf-idf vector for any code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db35ead-bb4e-4029-89f6-21c9554742f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import Similarity\n",
    "\n",
    "indexer = Similarity(\n",
    "    output_prefix=None,\n",
    "    corpus=tf_idf_model[BoW_matrix_for_code],\n",
    "    num_features=len(dictionary),\n",
    "    num_best=3,                  # let's see Top-3 result\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f59915-b1bd-4d8e-b2b0-70beb1dc7c46",
   "metadata": {},
   "source": [
    "The same `query` as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd0cb71-e025-49d7-a738-98c3643c0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"def foo(x):\n",
    "    if x > 5:\n",
    "        if x > 10:\n",
    "            return x + 1\n",
    "        else:\n",
    "            return x - 1\n",
    "    else:\n",
    "        if x < 0:\n",
    "            return x + 1\n",
    "        else:\n",
    "            return x - 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ce32f3d-0841-43cd-ba85-076299577604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5958, 0.8276320695877075),\n",
       " (19805, 0.7624242305755615),\n",
       " (19669, 0.7616549730300903)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer[tf_idf_model[dictionary.doc2bow(get_token_stream(query))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ecaf7-f8b2-4308-bc67-70c2dc246250",
   "metadata": {},
   "source": [
    "Compare to previous output(generated by the bag-of-word model):\n",
    "```\n",
    "[(19669, 0.7191814184188843),\n",
    " (19805, 0.705620288848877),\n",
    " (5958, 0.6945071220397949)]\n",
    "```\n",
    "\n",
    "The TF-IDF model give `corpus[5958]` high similarity score. Let's inspect this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff7812b4-d2c5-451d-bcaf-c5449b73f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def json_to_dict(x):\n",
      "    '''OAuthResponse class can't parse the JSON data with content-type\n",
      "-    text/html and because of a rubbish api, we can't just tell flask-oauthlib to treat it as json.'''\n",
      "    if x.find(b'callback') > -1:\n",
      "        # the rubbish api (https://graph.qq.com/oauth2.0/authorize) is handled here as special case\n",
      "        pos_lb = x.find(b'{')\n",
      "        pos_rb = x.find(b'}')\n",
      "        x = x[pos_lb:pos_rb + 1]\n",
      "\n",
      "    try:\n",
      "        if type(x) != str:  # Py3k\n",
      "            x = x.decode('utf-8')\n",
      "        return json.loads(x, encoding='utf-8')\n",
      "    except:\n",
      "        return x\n"
     ]
    }
   ],
   "source": [
    "print(corpus[5958])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85eb144-f7fb-462f-9c56-53b20d197f09",
   "metadata": {},
   "source": [
    "ðŸ¤”ï¸ Interstingly, this code contains a lot of `x`. I guess that's why the TF-IDF prefer this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b2cdbc8-1711-4368-aad8-fca5e677c601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 0.8880760906244697),\n",
       " ('1', 0.21030873215563448),\n",
       " ('>', 0.20264008358137425)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([\n",
    "    (dictionary.id2token[k], v)\n",
    "    for k, v in\n",
    "    tf_idf_model[dictionary.doc2bow(get_token_stream(query))]\n",
    "], key=lambda t: t[1], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1473d82-cfec-4170-ba4b-d452417fdc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 0.8798645408690431),\n",
       " ('find', 0.369319615724186),\n",
       " ('encoding', 0.12512606274449986)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([\n",
    "    (dictionary.id2token[k], v)\n",
    "    for k, v in\n",
    "    tf_idf_model[dictionary.doc2bow(get_token_stream(corpus[5958]))]\n",
    "], key=lambda t: t[1], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d8ddfd7-4159-4453-a256-44ab28e0f9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 0.8085718239321962),\n",
       " ('SUPPRESS_ERRORS', 0.4438771127891553),\n",
       " ('256', 0.2014353828377701)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([\n",
    "    (dictionary.id2token[k], v)\n",
    "    for k, v in\n",
    "    tf_idf_model[dictionary.doc2bow(get_token_stream(corpus[19669]))]\n",
    "], key=lambda t: t[1], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefa937-d09d-4bda-a05e-5341c22344fc",
   "metadata": {},
   "source": [
    "That's true, the `x` in `corpus[5958]` has higher word importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
